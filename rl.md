[TOC]

## Classical RL algorithms

**Human-level control through deep reinforcement learning [Deep Q-learning](https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf)**

**Deep Reinforcement Learning with Double Q-learning [Double Q-learning](https://arxiv.org/pdf/1509.06461)**

**Dueling Network Architectures for Deep Reinforcement Learning [Dueling Q-learning](https://arxiv.org/pdf/1511.06581)**

**Policy Gradient Methods for Reinforcement Learning with Function Approximation [Policy Gradient](https://proceedings.neurips.cc/paper_files/paper/1999/file/464d828b85b0bed98e80ade0a5c43b0f-Paper.pdf)**

**Continuous Control With Depp Reinforcement Learning [DDPG](https://arxiv.org/pdf/1509.02971)**

**Trust Region Policy Optimization [TRPO](https://arxiv.org/pdf/1502.05477)**

**Proximal Policy Optimization Algorithms [PPO](https://arxiv.org/pdf/1707.06347)**

**Soft Actor-Critic Algorithms and Applications [SAC](https://arxiv.org/pdf/1812.05905)**

**High-Dimensional continuous Control Using Generalized Advantage Estimation [GAE](https://arxiv.org/pdf/1506.02438)**

**Conservative Q-Learning for Offline Reinforcement Learning [Conservative Q-laerning](https://arxiv.org/pdf/2006.04779)**



## RL with LLM

**DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models [Deepseek GRPO](https://arxiv.org/pdf/2402.03300)**
* GRPO just replace the critic model back to average rewards with several samples, as MC. Itâ€™s specifically in math tasks, if we only use the result rewards, then no reward models, and MC is good, since not need another critic model, which may introduce some errors.

**DeepScaleR [deepscaler](https://pretty-radio-b75.notion.site/DeepScaleR-Surpassing-O1-Preview-with-a-1-5B-Model-by-Scaling-RL-19681902c1468005bed8ca303013a4e2)**



